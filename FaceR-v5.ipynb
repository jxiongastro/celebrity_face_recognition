{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABLE OF CONTENT\n",
    "\n",
    "0. [Introduction](#Introduction)<br>\n",
    "1. [Setting up](#Setting-Up)<br>\n",
    "2. [Training Stage](#Training-Stage)<br>\n",
    "    2.1 [Creating encodings dataset](#Creating-encodings-from-the-dataset)<br>\n",
    "    2.2 [Merging two dataset](#Merging-two-pickle-file-into-one)<br>\n",
    "3. [Image FR](#Face-recognition-on-images)<br>\n",
    "4. [Video FR](#Face-Recognition-on-a-Video)<br>\n",
    "    4.1 [Frequency](#1.-Using-frequency)<br>\n",
    "    4.2 [Area ratio](#2.-Using-area-ratio-relative-to-frame-size)<br>\n",
    "    4.3 [Central](#3.-Using-Euclidean-distance-from-the-centre)<br>\n",
    "5. [Quality Control](#Quality-Control)<br>\n",
    "    5.1 [Check FPS](#To-check-video-FPS)<br>\n",
    "    5.3 [Check Encodings Quality](#To-check-quality-of-the-dataset-encodings)<br>\n",
    "6. [Face Clustering](#Face-Clustering)<br>\n",
    "7. [Improvement Lists](#Improvement-Lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook providing the code and comments for the Face Recognition (FR) system for the Recommender System. The whole process of FR approach is highly depended on two library: dlib and face_recognition. The face_recognition library is built to ease the application of dlib in Python. Some of the code is also extracted from a blog specialised in computer vision. The creators of those two libraries have written articles about their libraries.<br>\n",
    "<br>\n",
    "References:<br>\n",
    "1)\tDlib by Davis King: <br>http://blog.dlib.net/2017/02/high-quality-face-recognition-with-deep.html <br>\n",
    "2)\tFace Recognition by Adam Geitgey: <br>https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78<br>\n",
    "3)  PyImageSearch by Adrian Rosebrock:<br> https://www.pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/<br>\n",
    "<br>\n",
    "It is to be understood that this face recognition system, although using the idea of deep neural network, is fairly different to the common process in using image classification CNNs such as Inception, VGG or ResNet. In fact, this system is a combination of CNNs, HoG (if you choose so), and kNN. Each models is use in combination in different stage of the face recognition.<br>\n",
    "<br>\n",
    "When an input image is feed into the system, a trained CNN (or HoG) detect the location of faces giving an output of a set of coordinates which represent the boxes around the faces. Next, the set of coordinates is preprocessed to crop out the face images which are input into another CNN that output a 128-D feature vector (known as encodings) for each face. These two CNN are trained differently to serve different purposes. <br>\n",
    "<br>\n",
    "The second CNN is trained in a method known as deep metric learning using triplet loss, where 3 images (2 of the same person, and 1 from a completely different person) are feeded as input. The training step will tweaks the neural network slightly so that it makes sure the distance of the encodings it generates for faces of the same person are slightly closer while making sure the distance of the encodings for faces of different persons are slightly further apart.  Any ten different pictures of the same person should give roughly the same encodings. This is the most important feature of the CNN that differentiate it from typical image classification CNN as it can produce encodings for faces that the CNN is not trained with.<br>\n",
    "<br>\n",
    "Well-known face recognition system by Google is also trained and implemented in this method.<br> FaceNet: https://www.cv-foundation.org/openaccess/content_cvpr_2015/app/1A_089.pdf<br>\n",
    "<br>\n",
    "To simplify the whole face recognition system, we will first create an imaginary situation. Let's say we want to identify the faces of person A and person B while remembering the fact that the CNN is never trained with them. First, we get a few pictures of person A, feed it into the face recognition system and got an encodings as output. We labelled the encodings with the name of person A.  Then, we do the same for person B. This is known as the 'training phase' for the kNN. After that, we have a new picture with a single face which we don't know whether it belongs to person A or person B. The picture is feeded into the system and got a new encoding as output. We compare the distance between that encoding with the previous encoding we have. The encoding that has the smallest distance to the new encodings is identified as a match, and labelled with the name attached to the previous encodings. This is known as the 'predicting stage'.<br>\n",
    "<br>\n",
    "The advantages of this methodology are the the retraining of the CNN (which takes 24 hours to get a decent model) is not required for new faces while the number of pictures for that 'training stage' is significant less that commonly needed. There are definitely room for improvement which is listed down in the last section of the notebook.<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up\n",
    "\n",
    "[Back to top](#TABLE-OF-CONTENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installations guide:<br>\n",
    "\n",
    "conda create --name <env_name> python=3.6<br>\n",
    "conda install matplotlib<br>\n",
    "conda install scikit-learn<br>\n",
    "pip install opencv-python<br>\n",
    "pip install dlib<br>\n",
    "pip install face_recognition<br>\n",
    "pip install imutils<br>\n",
    "\n",
    "Current version:<br>\n",
    "\n",
    "python == 3.6.6<br>\n",
    "matplotlib == 2.2.3<br>\n",
    "scikit-learn == 0.19.1 <br>\n",
    "opencv-python == 3.4.2.17<br>\n",
    "dlib == 19.15.0<br>\n",
    "face_recognition == 1.2.2<br>\n",
    "imutils == 0.4.6<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils import paths\n",
    "from matplotlib import pyplot as plt\n",
    "import face_recognition\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating encodings from the dataset\n",
    "\n",
    "[Back to top](#TABLE-OF-CONTENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the 'training stage' of the face recognition system.<br>\n",
    "<br>\n",
    "1) Create a parent folder which subfolder is named after the person.<br>\n",
    "2) Put all pictures for the specific person's faces in the subfolder with the corresponding names.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to input directory of faces + images (parent folder)\n",
    "input_path = \"C:/Users/Administrator/Desktop/Dataset/part_5\"\n",
    "# path to serialized db of facial encodings\n",
    "output_path = \"C:/Users/Administrator/Desktop/5_new_100.pickle\"\n",
    "# face detection model to use: either `hog` or `cnn`, CNN if using GPU, use HoG for CPU (faster but less accurate)\n",
    "detection_method = \"cnn\"\n",
    "\n",
    "print(\"Detection method:\" , detection_method+\".\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the paths to the input images in our dataset\n",
    "start = time.time()\n",
    "print(\"[INFO] quantifying faces...\")\n",
    "imagePaths = list(paths.list_images(input_path))\n",
    "\n",
    "# initialize the list of known encodings and known names\n",
    "knownEncodings = []\n",
    "knownNames = []\n",
    "\n",
    "# loop over the image paths\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    # extract the person name from the image path\n",
    "    print(\"[INFO] processing image {}/{}\".format(i + 1, len(imagePaths)))\n",
    "    name = imagePath.split(os.path.sep)[-2]\n",
    "    # load the input image and convert it from RGB (OpenCV ordering) to dlib ordering (RGB)\n",
    "    image = cv2.imread(imagePath)\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # detect the (x, y)-coordinates of the bounding boxes corresponding to each face in the input image\n",
    "    boxes = face_recognition.face_locations(rgb, model=detection_method)\n",
    "    for ((top, right, bottom, left)) in boxes:\n",
    "        print(\"boxing\")\n",
    "        # draw the predicted face name on the image\n",
    "        cv2.rectangle(rgb, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "        #print boxes to check unexpected detected object\n",
    "        print(\"Boxes:\", top, right, bottom, left)\n",
    "\n",
    "    # show the output image\n",
    "    # plt.imshow(rgb)\n",
    "    # plt.show()\n",
    "    #a = time.time()\n",
    "    # compute the facial embedding for the face\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "    #b = time.time()-a\n",
    "    #print(\"Image encoding time: \" + str(round(b,2)) + \" seconds\")\n",
    "\n",
    "    # loop over the encodings\n",
    "    for encoding in encodings:\n",
    "        # add each encoding + name to our set of known names and encodings\n",
    "        knownEncodings.append(encoding)\n",
    "        knownNames.append(name)\n",
    "\n",
    "# dump the facial encodings + names to disk\n",
    "print(\"[INFO] serializing encodings...\")\n",
    "data = {\"encodings\": knownEncodings, \"names\": knownNames}\n",
    "f = open(output_path, \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()\n",
    "\n",
    "print(\"[INFO] done encodings!\")\n",
    "total_time = time.time()-start\n",
    "print(\"Total time: \" + str(round(total_time,2)) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging two pickle file into one\n",
    "\n",
    "[Back to top](#TABLE-OF-CONTENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to previous serialized db of facial encodings to produce new one \n",
    "# if there is no previous encodings, just put None\n",
    "prev_output = None\n",
    "# path to serialized db of facial encodings\n",
    "new_output =  \"/Users/Elwin/Desktop/FaceR/face-recognition-opencv/encodings/enc_my_2_1_jitter_100.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is to merge two pickle file into one file\n",
    "if prev_output != None:\n",
    "    #previous_output\n",
    "    file_1 = open(prev_output, \"rb\")\n",
    "    pkl_1 = pickle.load(file_1)\n",
    "    print(\"pkl_1 names: \")\n",
    "    print(pkl_1[\"names\"][0],pkl_1[\"names\"][10],pkl_1[\"names\"][20],pkl_1[\"names\"][30],)\n",
    "    file_1.close()\n",
    "    #current output\n",
    "    file_2 = open(output_path, \"rb\")\n",
    "    pkl_2 = pickle.load(file_2)\n",
    "    print(\"pkl_2 names: \")\n",
    "    print(pkl_2[\"names\"][0],pkl_2[\"names\"][10],pkl_2[\"names\"][20],pkl_2[\"names\"][30])\n",
    "    file_2.close()\n",
    "    #new output\n",
    "    for i in pkl_2[\"encodings\"]:\n",
    "        pkl_1[\"encodings\"].append(i)\n",
    "    for i in pkl_2[\"names\"]:\n",
    "        pkl_1[\"names\"].append(i)\n",
    "    print(\"pkl_1 names: \")\n",
    "    print(pkl_1[\"names\"])\n",
    "    f = open(new_output, \"wb\")\n",
    "    f.write(pickle.dumps(pkl_1))\n",
    "    f.close()\n",
    "else:\n",
    "    print(\"No previous encodings!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(new_output, \"rb\")\n",
    "pkl_file = pickle.load(f)\n",
    "f.close()\n",
    "for i in enumerate(pkl_file[\"names\"]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognition on images\n",
    "\n",
    "[Back to top](#TABLE-OF-CONTENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to serialized db of facial encodings\n",
    "input_encodings = \"C:/Users/Administrator/Desktop/5_ori_1.pickle\"\n",
    "# path to input image\n",
    "image_path = \"C:/Users/Administrator/Desktop/test_2.jpg\"\n",
    "# just in case if updating face detection model to use: either `hog` or `cnn`\n",
    "detection_method = \"cnn\"\n",
    "# how much distance between faces to consider it a match, lower is more strict\n",
    "# based on investigation, Asian face seems to need stricter tolerance around 0.3 to 0.5\n",
    "tolerance = 0.5\n",
    "# changing the pyplot figure size\n",
    "plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# load the known faces and embeddings\n",
    "print(\"[INFO] loading encodings...\")\n",
    "data = pickle.loads(open(input_encodings, \"rb\").read())\n",
    "# time\n",
    "time_1 = time.time()\n",
    "time_x = time_1 - start\n",
    "print(\"Loading encodings: \" + str(round(time_x,4)) + \" seconds\")\n",
    "\n",
    "# load the input image and convert it from BGR to RGB\n",
    "image = cv2.imread(image_path)\n",
    "rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# detect the (x, y)-coordinates of the bounding boxes corresponding\n",
    "# to each face in the input image, then compute the facial embeddings\n",
    "# for each face\n",
    "print(\"[INFO] recognizing faces...\")\n",
    "boxes = face_recognition.face_locations(rgb, model=detection_method)\n",
    "print(boxes)\n",
    "# time\n",
    "time_2 = time.time()\n",
    "time_x = time_2 - time_1\n",
    "print(\"Locating faces: \" + str(round(time_x,4)) + \" seconds\")\n",
    "\n",
    "print(\"[INFO] encoding faces...\")\n",
    "encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "# time\n",
    "time_3 = time.time()\n",
    "time_x = time_3 - time_2\n",
    "print(\"Producing encodings: \" + str(round(time_x,4)) + \" seconds\")\n",
    "\n",
    "print(\"[INFO] comparing faces...\")\n",
    "# initialize the list of names for each face detected\n",
    "names = []\n",
    "# loop over the facial embeddings\n",
    "for encoding in encodings:\n",
    "    # attempt to match each face in the input image to our known encodings\n",
    "    matches = face_recognition.compare_faces(data[\"encodings\"], encoding, tolerance )\n",
    "    distance = face_recognition.face_distance(data[\"encodings\"], encoding)\n",
    "    name = \"Unknown\"\n",
    "    # check to see if we have found a match\n",
    "    if True in matches:\n",
    "        # find the indexes of all matched faces then initialize a\n",
    "        # dictionary to count the total number of times each face\n",
    "        # was matched\n",
    "        matchedIdxs = [i for (i, a) in enumerate(matches) if a]\n",
    "        print(matchedIdxs)\n",
    "        counts = {}\n",
    "\n",
    "        # loop over the matched indexes and maintain a count for\n",
    "        # each recognized face\n",
    "        for i in matchedIdxs:\n",
    "            name = data[\"names\"][i]\n",
    "            print(\"ID-name pair:\", i, name, round(distance[i],2))\n",
    "            counts[name] = counts.get(name, 0) + 1\n",
    "\n",
    "        # determine the recognized face with the largest number of\n",
    "        # votes (note: in the event of an unlikely tie Python will\n",
    "        # select first entry in the dictionary)\n",
    "        name = max(counts, key=counts.get)\n",
    "        print(\"Final name:\", name)\n",
    "    # update the list of names\n",
    "    names.append(name)\n",
    "# time\n",
    "time_4 = time.time()\n",
    "time_x = time_4 -time_3\n",
    "print(\"kNN: \" + str(round(time_x,4)) + \" seconds\")\n",
    "\n",
    "# loop over the recognized faces\n",
    "for ((top, right, bottom, left), name) in zip(boxes, names):\n",
    "    # draw the predicted face name on the image\n",
    "    cv2.rectangle(rgb, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "    y = top - 15 if top - 15 > 15 else top + 15\n",
    "    cv2.putText(rgb, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "# time\n",
    "time_5 = time.time()\n",
    "time_x= time_5-time_4\n",
    "print(\"Labeling images: \" + str(round(time_x,4)) + \" seconds\")\n",
    "\n",
    "# show the output image\n",
    "plt.imshow(rgb)\n",
    "plt.show()\n",
    "# time\n",
    "time_6 = time.time()\n",
    "time_x= time_6-time_5\n",
    "print(\"Display image: \" + str(round(time_x,4)) + \" seconds\")\n",
    "\n",
    "total_time = time.time()-start\n",
    "print(\"Total time: \" + str(round(total_time,4)) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition on a Video\n",
    "\n",
    "[Back to top](#TABLE-OF-CONTENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using frequency\n",
    "\n",
    "[Back to top](#TABLE-OF-CONTENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to serialized db of facial encodings\n",
    "input_encodings = \"C:/Users/Administrator/Desktop/my_1&2_combine_jitter_1.pickle\"\n",
    "# path to input video,\n",
    "video_path = \"C:/Users/Administrator/Desktop/meletop.mp4\"\n",
    "# path to output video, None if no output video required\n",
    "output_path = \"C:/Users/Administrator/Desktop/meletop_detected.mp4\"\n",
    "# whether or not to display output frame to screen, 1 display, 0 does not\n",
    "display = 0\n",
    "# just in case if updating face detection model to use: either `hog` or `cnn`\n",
    "detection_method = \"cnn\"\n",
    "# how much distance between faces to consider it a match, lower is more strict\n",
    "tolerance = 0.5\n",
    "# changing the pyplot figure size\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "# number of frame to skip every iteration\n",
    "skip = 25\n",
    "# path to text output for each names\n",
    "text_output = \"C:/Users/Administrator/Desktop/result_jitter_1.txt\"\n",
    "# path to saved output frame as image directory, None if not required.\n",
    "image_path = None\n",
    "# the threshold value for a frequency percentage to be deemed significant\n",
    "freq_tolerance = 0\n",
    "# frame speed for the output video\n",
    "frame_speed = 5\n",
    "\n",
    "print(\"Defined variable,\" , detection_method+\", skip=\"+str(skip)+\", frequency tolerance=\"+str(freq_tolerance) + \".\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# load the known faces and embeddings\n",
    "print(\"[INFO] loading encodings...\")\n",
    "data = pickle.loads(open(input_encodings, \"rb\").read())\n",
    "time_1 = time.time()\n",
    "time_x = time_1 - start\n",
    "print(\"Loading encodings: \" + str(round(time_x,4)) + \" seconds\")\n",
    "\n",
    "# initialize the pointer to the video file and the video writer\n",
    "print(\"[INFO] processing video...\")\n",
    "stream = cv2.VideoCapture(video_path)\n",
    "writer = None\n",
    "frame_num = 0\n",
    "freq = {}\n",
    "# loop over frames from the video file stream\n",
    "time_2 = time.time()\n",
    "time_x = time_2 - time_1\n",
    "print(\"Initialize video capture: \" + str(round(time_x,4)) + \" seconds\")\n",
    "while True:\n",
    "    time_1=time.time()\n",
    "    # grab the next frame\n",
    "    (grabbed, frame) = stream.read()\n",
    "    frame_num += 1\n",
    "    if frame_num % skip != 0:    #skipping through n number of frames\n",
    "        if frame is not None:   #as long as there is a frame\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    print(frame_num)\n",
    "    time_2 = time.time()\n",
    "    time_x = time_2 - time_1\n",
    "    print(\"Grabbing the right frame: \" + str(round(time_x,4)) + \" seconds\")\n",
    "    # convert the input frame from BGR to RGB then resize it to have\n",
    "    # we can use imutils resize function to change the width to 750px (to speedup processing) if needed\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    r = frame.shape[1] / float(rgb.shape[1])\n",
    "    \n",
    "    # detect the (x, y)-coordinates of the bounding boxes\n",
    "    # corresponding to each face in the input frame, then compute\n",
    "    # the facial embeddings for each face\n",
    "    boxes = face_recognition.face_locations(rgb, model=detection_method)\n",
    "    time_3 = time.time()\n",
    "    time_x = time_3 - time_2\n",
    "    print(\"Locating faces: \" + str(round(time_x,4)) + \" seconds\")\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "    time_4 = time.time()\n",
    "    time_x = time_4 - time_3\n",
    "    print(\"Encoding faces: \" + str(round(time_x,4)) + \" seconds\")\n",
    "    names = []\n",
    "    time_5 = time.time()\n",
    "    # loop over the facial embeddings\n",
    "    for encoding in encodings:\n",
    "        # attempt to match each face in the input image to our known\n",
    "        # encodings\n",
    "        matches = face_recognition.compare_faces(data[\"encodings\"], encoding, tolerance)\n",
    "        time_5 = time.time()\n",
    "        time_x = time_5 - time_4\n",
    "        print(\"kNN: \" + str(round(time_x,4)) + \" seconds\")\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        # check to see if we have found a match\n",
    "        if True in matches:\n",
    "            # find the indexes of all matched faces then initialize a\n",
    "            # dictionary to count the total number of times each face\n",
    "            # was matched\n",
    "            matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "            counts = {}\n",
    "\n",
    "            # loop over the matched indexes and maintain a count for\n",
    "            # each recognized face face\n",
    "            for i in matchedIdxs:\n",
    "                name = data[\"names\"][i]\n",
    "                counts[name] = counts.get(name, 0) + 1\n",
    "\n",
    "            # determine the recognized face with the largest number\n",
    "            # of votes (note: in the event of an unlikely tie Python\n",
    "            # will select first entry in the dictionary)\n",
    "            name = max(counts, key=counts.get)\n",
    "            \n",
    "        # update the list of names\n",
    "        names.append(name)\n",
    "    time_6 = time.time()\n",
    "    time_x = time_6 - time_5\n",
    "    print(\"Getting name from kNN: \" + str(round(time_x,4)) + \" seconds\")\n",
    "    \n",
    "    # introducing a frequency counter after the faces are identified\n",
    "    \n",
    "    # loop over the recognized faces\n",
    "    for ((top, right, bottom, left), name) in zip(boxes, names):\n",
    "        # rescale the face coordinates\n",
    "        top = int(top * r)\n",
    "        right = int(right * r)\n",
    "        bottom = int(bottom * r)\n",
    "        left = int(left * r)\n",
    "        \n",
    "        # draw the predicted face name on the image\n",
    "        cv2.rectangle(rgb, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        y = top - 15 if top - 15 > 15 else top + 15\n",
    "        cv2.putText(rgb, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "        \n",
    "        # draw the predicted face name on the image to save using OpenCV\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        y = top - 15 if top - 15 > 15 else top + 15\n",
    "        cv2.putText(frame, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "        \n",
    "        freq[name] = freq.get(name, 0) + 1\n",
    "        \n",
    "# The end of processing the frame images\n",
    "# Now back to output video\n",
    "\n",
    "    # if the video writer is None *AND* we are supposed to write\n",
    "    # the output video to disk initialize the writer\n",
    "    if writer is None and output_path is not None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        writer = cv2.VideoWriter(output_path, fourcc, frame_speed ,(frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "    # if the writer is not None, write the frame with recognized faces to disk\n",
    "    if writer is not None:\n",
    "        writer.write(frame)\n",
    "        time_7 = time.time()\n",
    "        time_x = time_7 - time_6\n",
    "        print(\"Output video: \" + str(round(time_x,4)) + \" seconds\")\n",
    "\n",
    "    # check to see if we are supposed to display the output frame to the screen\n",
    "    if display > 0:\n",
    "        plt.imshow(rgb)\n",
    "        plt.show()\n",
    "        \n",
    "    # check to see if we are supposed to save the output frame as images\n",
    "    if image_path != None\n",
    "        image_full_path = image_path+str(frame_num)+\".png\"\n",
    "        #cv2.imwrite(image_full_path, frame)\n",
    "\n",
    "#output the result into text file\n",
    "f = open(text_output,'a')\n",
    "iter_num = frame_num/skip\n",
    "for key, val in freq.items():\n",
    "    freq_ratio = round(val/iter_num*100,2)\n",
    "    if key is \"Unknown\":\n",
    "        continue\n",
    "    elif freq_ratio > freq_tolerance:\n",
    "        f.write(str(key)+\" \"+str(freq_ratio)+\"%\" +'\\n')\n",
    "        \n",
    "f.write(\"Total number of frames: \"+str(frame_num-1)+'\\n')\n",
    "f.write(\"Total number of iterations: \"+str(iter_num)+'\\n')\n",
    "f.close()\n",
    "time_8 = time.time()\n",
    "time_x = time_8 - time_7\n",
    "print(\"Output text file: \" + str(round(time_x,4)) + \" seconds\")\n",
    "\n",
    "# close the video file pointers\n",
    "stream.release()\n",
    "\n",
    "# check to see if the video writer point needs to be released\n",
    "if writer is not None:\n",
    "    writer.release()\n",
    "    \n",
    "total_time = time.time()-start\n",
    "print(\"Total time: \" + str(round(total_time,2)) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using area ratio relative to frame size\n",
    "\n",
    "[Back to top](#TABLE-OF-CONTENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These codes are not as detailed as the video FR using frequency, some changes are necessary to be as functional as the code that uses frequency."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# path to serialized db of facial encodings\n",
    "input_encodings = \"/Users/Elwin/Desktop/FaceR/face-recognition-opencv/encodings/enc_my_2.pickle\"\n",
    "# path to input video,\n",
    "video_path = \"/Users/Elwin/Desktop/FaceR/face-recognition-opencv/test_sample/video_example/awal_scha2.mp4\"\n",
    "# path to output video, None if no output video required\n",
    "output_path = None\n",
    "# whether or not to display output frame to screen, 1 display, 0 does not\n",
    "display = 1\n",
    "# just in case if updating face detection model to use: either `hog` or `cnn`\n",
    "detection_method = \"hog\"\n",
    "# how much distance between faces to consider it a match, lower is more strict\n",
    "tolerance = 0.5\n",
    "# changing the pyplot figure size\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "# number of frame to skip every iteration\n",
    "skip = 30\n",
    "print(\"Defined variable and detecting with\" , detection_method + \".\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# path to text output for each names\n",
    "text_output = \"/Users/Elwin/Desktop/FaceR/face-recognition-opencv/result_area.txt\"\n",
    "\n",
    "start = time.time()\n",
    "# load the known faces and embeddings\n",
    "print(\"[INFO] loading encodings...\")\n",
    "data = pickle.loads(open(input_encodings, \"rb\").read())\n",
    "\n",
    "# initialize the pointer to the video file and the video writer\n",
    "print(\"[INFO] processing video...\")\n",
    "stream = cv2.VideoCapture(video_path)\n",
    "writer = None\n",
    "frame_num = 0\n",
    "# loop over frames from the video file stream\n",
    "while True:\n",
    "    # grab the next frame\n",
    "    (grabbed, frame) = stream.read()\n",
    "    frame_num += 1\n",
    "    if frame_num % skip != 0:    #skipping through n number of frames\n",
    "        if frame is not None:   #as long as there is a frame\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    print(frame_num)\n",
    "    # convert the input frame from BGR to RGB then resize it to have\n",
    "    # we can use imutils resize function to change the width to 750px (to speedup processing) if needed\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    r = frame.shape[1] / float(rgb.shape[1])\n",
    "    frame_area = frame.shape[0]*frame.shape[1]\n",
    "\n",
    "    # detect the (x, y)-coordinates of the bounding boxes\n",
    "    # corresponding to each face in the input frame, then compute\n",
    "    # the facial embeddings for each face\n",
    "    boxes = face_recognition.face_locations(rgb, model=detection_method)\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "    names = []\n",
    "    text_num = []\n",
    "\n",
    "    # loop over the facial embeddings\n",
    "    for encoding in encodings:\n",
    "        # attempt to match each face in the input image to our known\n",
    "        # encodings\n",
    "        matches = face_recognition.compare_faces(data[\"encodings\"], encoding, tolerance)\n",
    "        name = \"Unknown\"\n",
    "        print(\"Just before append:\", frame_num)\n",
    "        text_num.append(frame_num)\n",
    "\n",
    "        # check to see if we have found a match\n",
    "        if True in matches:\n",
    "            # find the indexes of all matched faces then initialize a\n",
    "            # dictionary to count the total number of times each face\n",
    "            # was matched\n",
    "            matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "            counts = {}\n",
    "\n",
    "            # loop over the matched indexes and maintain a count for\n",
    "            # each recognized face face\n",
    "            for i in matchedIdxs:\n",
    "                name = data[\"names\"][i]\n",
    "                counts[name] = counts.get(name, 0) + 1\n",
    "\n",
    "            # determine the recognized face with the largest number\n",
    "            # of votes (note: in the event of an unlikely tie Python\n",
    "            # will select first entry in the dictionary)\n",
    "            name = max(counts, key=counts.get)\n",
    "\n",
    "        # update the list of names\n",
    "        names.append(name)\n",
    "    \n",
    "    print(text_num)\n",
    "    # loop over the recognized faces\n",
    "    for ((top, right, bottom, left), name, num) in zip(boxes, names, text_num):\n",
    "        # rescale the face coordinates\n",
    "        top = int(top * r)\n",
    "        right = int(right * r)\n",
    "        bottom = int(bottom * r)\n",
    "        left = int(left * r)\n",
    "        \n",
    "        # draw the predicted face name on the image\n",
    "        cv2.rectangle(rgb, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        y = top - 15 if top - 15 > 15 else top + 15\n",
    "        cv2.putText(rgb, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "        \n",
    "        #output the result into text file\n",
    "        print(\"Num of text_num:\", num)\n",
    "        area_ratio = (top-bottom)*(left-right)*100/frame_area\n",
    "        f = open(text_output,'a')\n",
    "        f.write(str(num)+\" \"+str(name)+\" \"+str(round(area_ratio,2))+\"%\" +'\\n')\n",
    "        f.close()\n",
    "        \n",
    "# The end of processing the frame images\n",
    "# Now back to output video\n",
    "\n",
    "    # if the video writer is None *AND* we are supposed to write\n",
    "    # the output video to disk initialize the writer\n",
    "    if writer is None and output_path is not None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        writer = cv2.VideoWriter(outout_path, fourcc, 24,(frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "    # if the writer is not None, write the frame with recognized faces to disk\n",
    "    if writer is not None:\n",
    "        writer.write(frame)\n",
    "\n",
    "    # check to see if we are supposed to display the output frame to the screen\n",
    "    if display > 0:\n",
    "        plt.imshow(rgb)\n",
    "        plt.show()\n",
    "\n",
    "# close the video file pointers\n",
    "stream.release()\n",
    "\n",
    "# check to see if the video writer point needs to be released\n",
    "if writer is not None:\n",
    "    writer.release()\n",
    "    \n",
    "total_time = time.time()-start\n",
    "print(\"Total time: \" + str(round(total_time,2)) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Euclidean distance from the centre\n",
    "\n",
    "[Back to top](#TABLE-OF-CONTENT)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# path to text output for each names\n",
    "text_output = \"/Users/Elwin/Desktop/FaceR/face-recognition-opencv/result_centre.txt\"\n",
    "    \n",
    "start = time.time()\n",
    "# load the known faces and embeddings\n",
    "print(\"[INFO] loading encodings...\")\n",
    "data = pickle.loads(open(input_encodings, \"rb\").read())\n",
    "\n",
    "# initialize the pointer to the video file and the video writer\n",
    "print(\"[INFO] processing video...\")\n",
    "stream = cv2.VideoCapture(video_path)\n",
    "writer = None\n",
    "frame_num = 0\n",
    "freq = {}\n",
    "# loop over frames from the video file stream\n",
    "while True:\n",
    "    # grab the next frame\n",
    "    (grabbed, frame) = stream.read()\n",
    "    frame_num += 1\n",
    "    if frame_num % skip != 0:    #skipping through n number of frames\n",
    "        if frame is not None:   #as long as there is a frame\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    print(frame_num)\n",
    "    # convert the input frame from BGR to RGB then resize it to have\n",
    "    # we can use imutils resize function to change the width to 750px (to speedup processing) if needed\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    r = frame.shape[1] / float(rgb.shape[1])\n",
    "    \n",
    "    #Added some geometrical feature for measuring Euclidean distance from the centre.\n",
    "    y_cen = frame.shape[0]/2\n",
    "    x_cen = frame.shape[1]/2\n",
    "    rad = frame.shape[0]/3\n",
    "    \n",
    "    # a need to define the Euclidean distance function. Will return True if within radius.\n",
    "    def euclidean(x,y):\n",
    "        dX = x_cen - x\n",
    "        dY = y_cen - y\n",
    "        L = sqrt(dX * dX + dY * dY)\n",
    "        if L < rad:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "    # detect the (x, y)-coordinates of the bounding boxes\n",
    "    # corresponding to each face in the input frame, then compute\n",
    "    # the facial embeddings for each face\n",
    "    boxes = face_recognition.face_locations(rgb, model=detection_method)\n",
    "    \n",
    "    # added box_num\n",
    "    box_num = 0\n",
    "    box_del = []\n",
    "    #Added a loop to eliminate out of range boxes using Euclidean distance\n",
    "    for (top, right, bottom, left) in boxes:\n",
    "        print(boxes)\n",
    "        d1 = euclidean(right,top)         \n",
    "        d2 = euclidean(right,bottom)\n",
    "        d3 = euclidean(left,top)\n",
    "        d4 = euclidean(left,bottom)\n",
    "        x_frame_cen = (left+right)/2\n",
    "        y_frame_cen = (top+bottom)/2\n",
    "        d5 = euclidean(x_frame_cen,y_frame_cen)\n",
    "        total = d1+d2+d3+d4+d5\n",
    "        print(total)\n",
    "        if total < 1:\n",
    "            box_del.append(box_num)\n",
    "            print((top, right, bottom, left))\n",
    "        else:\n",
    "            box_num +=1\n",
    "    \n",
    "    for i in box_del:\n",
    "        del boxes[i]\n",
    "    \n",
    "    print(boxes)\n",
    "    \n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "    names = []\n",
    "\n",
    "    # loop over the facial embeddings\n",
    "    for encoding in encodings:\n",
    "        # attempt to match each face in the input image to our known\n",
    "        # encodings\n",
    "        matches = face_recognition.compare_faces(data[\"encodings\"], encoding, tolerance)\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        # check to see if we have found a match\n",
    "        if True in matches:\n",
    "            # find the indexes of all matched faces then initialize a\n",
    "            # dictionary to count the total number of times each face\n",
    "            # was matched\n",
    "            matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "            counts = {}\n",
    "\n",
    "            # loop over the matched indexes and maintain a count for\n",
    "            # each recognized face face\n",
    "            for i in matchedIdxs:\n",
    "                name = data[\"names\"][i]\n",
    "                counts[name] = counts.get(name, 0) + 1\n",
    "\n",
    "            # determine the recognized face with the largest number\n",
    "            # of votes (note: in the event of an unlikely tie Python\n",
    "            # will select first entry in the dictionary)\n",
    "            name = max(counts, key=counts.get)\n",
    "            \n",
    "        # update the list of names\n",
    "        names.append(name)\n",
    "    \n",
    "    # introducing a frequency counter after the faces are identified\n",
    "    \n",
    "    # loop over the recognized faces\n",
    "    for ((top, right, bottom, left), name) in zip(boxes, names):\n",
    "        # rescale the face coordinates\n",
    "        top = int(top * r)\n",
    "        right = int(right * r)\n",
    "        bottom = int(bottom * r)\n",
    "        left = int(left * r)\n",
    "        \n",
    "        # draw the predicted face name on the image\n",
    "        cv2.rectangle(rgb, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        y = top - 15 if top - 15 > 15 else top + 15\n",
    "        cv2.putText(rgb, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "        \n",
    "        freq[name] = freq.get(name, 0) + 1\n",
    "        \n",
    "# The end of processing the frame images\n",
    "# Now back to output video\n",
    "\n",
    "    # if the video writer is None *AND* we are supposed to write\n",
    "    # the output video to disk initialize the writer\n",
    "    if writer is None and output_path is not None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        writer = cv2.VideoWriter(outout_path, fourcc, 24,(frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "    # if the writer is not None, write the frame with recognized faces to disk\n",
    "    if writer is not None:\n",
    "        writer.write(frame)\n",
    "\n",
    "    # check to see if we are supposed to display the output frame to the screen\n",
    "    if display > 0:\n",
    "        plt.imshow(rgb)\n",
    "        plt.show()\n",
    "\n",
    "#output the result into text file\n",
    "f = open(text_output,'a')\n",
    "iter_num = frame_num/skip\n",
    "for key, val in freq.items():\n",
    "    freq_ratio = round(val/iter_num*100,2)\n",
    "    f.write(str(key)+\" \"+str(freq_ratio)+\"%\" +'\\n')\n",
    "f.write(\"Total number of frames: \"+str(frame_num-1)+'\\n')\n",
    "f.close()\n",
    "\n",
    "# close the video file pointers\n",
    "stream.release()\n",
    "\n",
    "# check to see if the video writer point needs to be released\n",
    "if writer is not None:\n",
    "    writer.release()\n",
    "    \n",
    "total_time = time.time()-start\n",
    "print(\"Total time: \" + str(round(total_time,2)) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To check video FPS\n",
    "\n",
    "[Back to top](#TABLE-OF-CONTENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# path to input video,\n",
    "video_path = \"/Users/Elwin/Desktop/FaceR/face-recognition-opencv/demo/demo_my_outlier/jep.mp4\"\n",
    "\n",
    "# initialize the pointer to the video file\n",
    "stream = cv2.VideoCapture(video_path)\n",
    "fps = stream.get(cv2.CAP_PROP_FPS)\n",
    "length = stream.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "# close the video file pointers\n",
    "stream.release()\n",
    "print(\"FPS: \" + str(fps))\n",
    "print(\"Number of frames: \" + str(length))\n",
    "    \n",
    "total_time = time.time()-start\n",
    "print(\"Total time: \" + str(round(total_time,2)) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To check quality of the dataset encodings\n",
    "\n",
    "[Back to top](#TABLE-OF-CONTENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to serialized db of facial encodings\n",
    "check = \"C:/Users/Administrator/Desktop/5_ori_1.pickle\"\n",
    "\n",
    "#previous_output\n",
    "file_1 = open(check, \"rb\")\n",
    "pkl_1 = pickle.load(file_1)\n",
    "file_1.close()\n",
    "test_count = 0\n",
    "for i,j in enumerate(pkl_1[\"encodings\"]):        \n",
    "    encoding = j\n",
    "    name = pkl_1[\"names\"][i]\n",
    "\n",
    "    matches = face_recognition.face_distance(pkl_1[\"encodings\"], encoding)\n",
    "\n",
    "    # to check if similar names has too big difference\n",
    "    for k,m in enumerate(matches):\n",
    "        if m < 0.4:\n",
    "            if i != k and pkl_1[\"names\"][k] != name:\n",
    "                test_count +=1\n",
    "                print(\"Fault\",test_count,\":\",i, pkl_1[\"names\"][i])\n",
    "                print(k, pkl_1[\"names\"][k],round(m,2), \"Very confident\")\n",
    "                print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = None\n",
    "number = 0\n",
    "for i, t in enumerate(pkl_1[\"names\"]):\n",
    "    if t != name:\n",
    "        number = 1\n",
    "        name = t\n",
    "    else: \n",
    "        number +=1\n",
    "    if i == 69:\n",
    "        print(i, t, number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = open(output_path, \"rb\")\n",
    "pkl_1 = pickle.load(file_1)\n",
    "file_1.close()\n",
    "print(len(pkl_1[\"encodings\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Clustering\n",
    "\n",
    "[Back to top](#TABLE-OF-CONTENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to input directory of faces + images\n",
    "input_path = \"\"\n",
    "# path to serialized db of facial encodings\n",
    "output_path = \"/Users/Elwin/Desktop/FaceR/face-recognition-opencv/encodings/enc_clus_2.pickle\"\n",
    "# face detection model to use: either `hog` or `cnn`\n",
    "detection_method = \"cnn\"\n",
    "# changing the pyplot figure size\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "# path to serialized db of facial encodings\n",
    "input_encodings = output_path\n",
    "# 1 to display the image, 0 does not \n",
    "display = 0\n",
    "# of parallel jobs to run (-1 will use all CPUs)\n",
    "job = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the paths to the input images in our dataset, then initialize\n",
    "# out data list (which we'll soon populate)\n",
    "start = time.time()\n",
    "print(\"[INFO] quantifying faces...\")\n",
    "imagePaths = list(paths.list_images(input_path))\n",
    "data = []\n",
    "\n",
    "# loop over the image paths\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "# load the input image and convert it from RGB (OpenCV ordering)\n",
    "# to dlib ordering (RGB)\n",
    "    print(\"[INFO] processing image {}/{}\".format(i + 1, len(imagePaths)))\n",
    "    print(imagePath)\n",
    "    image = cv2.imread(imagePath)\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# detect the (x, y)-coordinates of the bounding boxes corresponding to each face in the input image\n",
    "    boxes = face_recognition.face_locations(rgb, model=detection_method)\n",
    "    for ((top, right, bottom, left)) in boxes:\n",
    "        # draw the predicted face name on the image\n",
    "        cv2.rectangle(rgb, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "    # show the output image\n",
    "    if display > 0:\n",
    "        plt.imshow(rgb)\n",
    "        plt.show()\n",
    "    \n",
    "    # compute the facial embedding for the face\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "\n",
    "    # build a dictionary of the image path, bounding box location, and facial encodings for the current image\n",
    "    d = [{\"imagePath\": imagePath, \"loc\": box, \"encoding\": enc} for (box, enc) in zip(boxes, encodings)]\n",
    "    data.extend(d)\n",
    "\n",
    "# dump the facial encodings data to disk\n",
    "print(\"[INFO] serializing encodings...\")\n",
    "f = open(output_path, \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()\n",
    "\n",
    "print(\"[INFO] done encodings!\")\n",
    "total_time = time.time()-start\n",
    "print(\"Total time: \" + str(round(total_time,2)) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from imutils import build_montages\n",
    "\n",
    "# load the serialized face encodings + bounding box locations from disk, to cluster them\n",
    "print(\"[INFO] loading encodings...\")\n",
    "data = pickle.loads(open(input_encodings, \"rb\").read())\n",
    "data = np.array(data)\n",
    "encodings = [d[\"encoding\"] for d in data]\n",
    "\n",
    "# cluster the embeddings\n",
    "print(\"[INFO] clustering...\")\n",
    "clt = DBSCAN(metric=\"euclidean\", n_jobs=job)\n",
    "clt.fit(encodings)\n",
    "\n",
    "# determine the total number of unique faces found in the dataset\n",
    "labelIDs = np.unique(clt.labels_)\n",
    "numUniqueFaces = len(np.where(labelIDs > -1)[0])\n",
    "print(\"[INFO] # unique faces: {}\".format(numUniqueFaces))\n",
    "\n",
    "# loop over the unique face integers\n",
    "for labelID in labelIDs:\n",
    "    # find all indexes into the `data` array that belong to the\n",
    "    # current label ID, then randomly sample a maximum of 25 indexes\n",
    "    # from the set\n",
    "    print(\"[INFO] faces for face ID: {}\".format(labelID))\n",
    "    idxs = np.where(clt.labels_ == labelID)[0]\n",
    "    idxs = np.random.choice(idxs, size=min(25, len(idxs)), replace=False)\n",
    "\n",
    "    # initialize the list of faces to include in the montage\n",
    "    faces = []\n",
    "\n",
    "# loop over the sampled indexes\n",
    "    for i in idxs:\n",
    "        # load the input image and extract the face ROI\n",
    "        image = cv2.imread(data[i][\"imagePath\"])\n",
    "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        (top, right, bottom, left) = data[i][\"loc\"]\n",
    "        face = rgb[top:bottom, left:right]\n",
    "\n",
    "        # force resize the face ROI to 96x96 and then add it to the\n",
    "        # faces montage list\n",
    "        face = cv2.resize(face, (96, 96))\n",
    "        faces.append(face)\n",
    "\n",
    "    # create a montage using 96x96 \"tiles\" with 5 rows and 5 columns\n",
    "    montage = build_montages(faces, (96, 96), (5, 5))[0]\n",
    "\n",
    "    # show the output montage\n",
    "    title = \"Face ID #{}\".format(labelID)\n",
    "    title = \"Unknown Faces\" if labelID == -1 else title\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = [20, 20]\n",
    "    # show the output image\n",
    "    plt.imshow(montage)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement Lists\n",
    "\n",
    "[Back to top](#TABLE-OF-CONTENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) It is understood that k-NN might be slow due to its lazy learning nature when the number of encodings is large. Thus, the next step is probably using approximate search to speed up the process (or even use another classifying algorithm altogether).<br><br>\n",
    "2)\tThe tolerance has been tweaked around to see its effect on the result of the classifier. It is shown that lower tolerance i.e. stricter produce more accurate result, where smaller value of tolerance will result in the classifier being unable to identify any faces. <br><br>\n",
    "3)\tAfter a few experimentations, the current code is still maintained, but a possible improvement is giving back the name “unknown” when the number of votes does not reach a threshold value to reduce the number of false positive (where increased number of false positives is the result of reducing the number of false negatives by having low tolerance). This method can be implemented quickly but has no proper evaluation on whether it will actually improve the accuracy.<br> <br>\n",
    "4)\tThe quality of encodings is quantified by comparing the distance of an encoding of a dataset to the other encodings of the same dataset. It can be seen that the instances where a pair of encodings from two different people having a distance lower than 0.5 is more often in Asian dataset in comparison to Caucasian dataset. This shows that the distance variability of the encodings in Asian dataset might not be as great as Caucasian dataset, leading to an assumption that the encoding-producing CNN network is the reason the face recognition network does not work well with Asian faces.<br><br>\n",
    "5)\tAn Asian dataset has been found provided by Microsoft DeepGlint which has 2 datasets, one is a a cleaned smaller version of MS-Celeb-1M dataset (3.9M images, 87k classes), another is Asian Celeb dataset (2.8M images, 94k classes). The Asian Celeb dataset has been used to train the encodings CNN, but the distance variability is actually worse than the original model, even after second retraining after resizing the dataset from 400x400 to 150x150 to match the original Caucasian dataset which the CNN is initially trained with.<br> <br>\n",
    "6)\tBased on searches on existing face recognition algorithms, it is found that FaceNet (which is developed by Google Research) has the highest accuracy of 99.63% in Labelled Faces in the Wild (LFW) dataset evaluation. However, it should be noted that the TensorFlow (by David Sandberg) and OpenFace implementations of FaceNet have varied accuracy due to the differences in training set (and also the quality of the implementations might affect the result in some way that has not been clearly understood). Notable algorithms are DeepFace by Facebook, VGG Face by VGG Oxford Group, and Fusion (which is also developed by Facebook).<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
